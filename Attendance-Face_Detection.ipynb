{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d5ec42-5b4d-4394-98c3-f1922ace5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c3e719-c93b-4745-8f85-2d49bee56ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83154588-9e93-49f2-89ea-cdffb1831267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a1696b-a3a3-44bf-b643-c19bd24fe6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgChimya_bgr = face_recognition.load_image_file('train_img_path/momo.jpeg')\n",
    "imgChimya_rgb = cv2.cvtColor(imgChimya_bgr, cv2.COLOR_BGR2RGB)\n",
    "# cv2.imshow('bgr',imgChimya_bgr)\n",
    "# cv2.imshow('rgb',imgChimya_rgb)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f6770a-177c-4a20-a9cd-becf1e906595",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[248, 242, 229],\n",
       "        [248, 242, 229],\n",
       "        [248, 242, 229],\n",
       "        ...,\n",
       "        [252, 251, 237],\n",
       "        [252, 251, 237],\n",
       "        [252, 251, 237]],\n",
       "\n",
       "       [[248, 242, 229],\n",
       "        [248, 242, 229],\n",
       "        [248, 242, 229],\n",
       "        ...,\n",
       "        [252, 251, 237],\n",
       "        [252, 251, 237],\n",
       "        [252, 251, 237]],\n",
       "\n",
       "       [[248, 242, 229],\n",
       "        [248, 242, 229],\n",
       "        [248, 242, 229],\n",
       "        ...,\n",
       "        [252, 251, 237],\n",
       "        [252, 251, 237],\n",
       "        [252, 251, 237]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[213, 227, 225],\n",
       "        [207, 221, 219],\n",
       "        [205, 219, 217],\n",
       "        ...,\n",
       "        [197, 214, 211],\n",
       "        [171, 188, 185],\n",
       "        [157, 174, 171]],\n",
       "\n",
       "       [[238, 252, 250],\n",
       "        [213, 227, 225],\n",
       "        [202, 216, 214],\n",
       "        ...,\n",
       "        [229, 246, 243],\n",
       "        [188, 205, 202],\n",
       "        [163, 180, 177]],\n",
       "\n",
       "       [[201, 215, 213],\n",
       "        [203, 217, 215],\n",
       "        [212, 226, 224],\n",
       "        ...,\n",
       "        [241, 255, 255],\n",
       "        [184, 201, 198],\n",
       "        [154, 171, 168]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgChimya = face_recognition.load_image_file('train_img_path/momo.jpeg')\n",
    "imgChimya = cv2.cvtColor(imgChimya, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "face = face_recognition.face_locations(imgChimya)[0]\n",
    "copy = imgChimya.copy()\n",
    "\n",
    "cv2.rectangle(copy, (face[3], face[0]), (face[1], face[2]), (255, 0, 255), 2)\n",
    "# cv2_imshow(imgChimya)\n",
    "# cv2.imshow('box',copy)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c809ee7-78eb-4c38-a62b-c0190621ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chimya_encoding = face_recognition.face_encodings(imgChimya)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f150de96-9acd-474b-bed3-af5636401adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True]\n"
     ]
    }
   ],
   "source": [
    "test = face_recognition.load_image_file('test_img_path/momo_test2.jpeg')\n",
    "test = cv2.cvtColor(test, cv2.COLOR_BGR2RGB)\n",
    "test_encode = face_recognition.face_encodings(test)[0]\n",
    "print(face_recognition.compare_faces([train_chimya_encoding], test_encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ceda483-2ca3-42b2-9e4c-76234b2e1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7060237-bc26-40cf-9ba9-412cd0e812ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chimya.png\n",
      "train_img_path/chimya.png\n",
      "girish.jpg\n",
      "train_img_path/girish.jpg\n",
      "momo.jpeg\n",
      "train_img_path/momo.jpeg\n"
     ]
    }
   ],
   "source": [
    "train_path = 'train_img_path'\n",
    "try_path = os.listdir(train_path)\n",
    "encode_face_List = []\n",
    "\n",
    "for file in try_path:\n",
    "  if(file.endswith(\".jpeg\") or file.endswith(\".jpg\") or file.endswith(\".png\")):\n",
    "    print(file)\n",
    "    img_path = train_path+'/'+file \n",
    "    print(img_path)\n",
    "\n",
    "    train_img = face_recognition.load_image_file(img_path)\n",
    "    train_img = cv2.cvtColor(train_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    train_encoding = face_recognition.face_encodings(train_img)[0]\n",
    "    encode_face_List.append(train_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb7fdad-44de-4a78-9be4-c6ad2e90d586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2022-04-05 16:13:16.188876\n",
      "date:  05/04/2022\n",
      "time:  16:13:16\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "date = now.strftime(\"%d/%m/%Y\") \n",
    "time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"date: \", date)\n",
    "print(\"time: \", time)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f83a84-31b6-4579-bce6-700372ad80fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05-04-2022\n",
      "16:13:18\n"
     ]
    }
   ],
   "source": [
    "from pytz import timezone \n",
    "from datetime import datetime\n",
    "\n",
    "ind_date = datetime.now(timezone(\"Asia/Kolkata\")).strftime('%d-%m-%Y')\n",
    "ind_time = datetime.now(timezone(\"Asia/Kolkata\")).strftime('%H:%M:%S')\n",
    "print(ind_date)\n",
    "print(ind_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13121498-0a74-4130-9bd5-8f83f4368750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markAttendance(name):\n",
    "  with open('Attendance.csv', 'r+') as f:\n",
    "    dataList = f.readlines()\n",
    "    nameList = []\n",
    "    for line in dataList[1:]:\n",
    "      entry = line.split(',')\n",
    "      nameList.append(entry[0])\n",
    "    if name not in nameList:\n",
    "      ind_date = datetime.now(timezone(\"Asia/Kolkata\")).strftime('%d-%m-%Y')\n",
    "      ind_time = datetime.now(timezone(\"Asia/Kolkata\")).strftime('%H:%M:%S')\n",
    "      f.writelines(f'\\n{name}, {ind_time}, {ind_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f864751-9798-4f31-ba1d-50112e1dbde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal Dims:  (1014, 1280, 3)\n",
      "Resized Dims:  (304, 384, 3)\n"
     ]
    }
   ],
   "source": [
    "imgS = cv2.imread('train_img_path/momo.jpeg')\n",
    "print('Orignal Dims: ', imgS.shape)\n",
    "\n",
    "scale_percent = 30\n",
    "width = int(imgS.shape[1]*scale_percent/100)\n",
    "height = int(imgS.shape[0]*scale_percent/100)\n",
    "dim = (width, height)\n",
    "\n",
    "resized = cv2.resize(imgS, dim)\n",
    "print('Resized Dims: ', resized.shape)\n",
    "# cv2.imshow('resized',resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d062ff70-dc59-4963-8ed5-f45c9632dde0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(201, 469, 387, 283)]\n",
      "[{'chin': [(285, 269), (281, 290), (279, 312), (281, 334), (289, 355), (305, 371), (326, 382), (349, 390), (374, 394), (400, 396), (420, 390), (438, 381), (449, 367), (455, 350), (459, 333), (461, 315), (461, 299)], 'left_eyebrow': [(314, 248), (331, 238), (350, 237), (369, 242), (386, 251)], 'right_eyebrow': [(401, 256), (417, 254), (431, 255), (445, 258), (451, 271)], 'nose_bridge': [(392, 269), (392, 277), (392, 284), (391, 293)], 'nose_tip': [(366, 309), (375, 311), (385, 314), (394, 315), (402, 316)], 'left_eye': [(333, 267), (344, 263), (354, 265), (364, 272), (353, 272), (342, 271)], 'right_eye': [(406, 281), (418, 276), (430, 279), (437, 287), (428, 288), (417, 285)], 'top_lip': [(343, 334), (359, 329), (373, 326), (380, 330), (389, 330), (400, 338), (411, 350), (406, 348), (388, 340), (379, 339), (371, 336), (349, 335)], 'bottom_lip': [(411, 350), (397, 352), (386, 351), (377, 349), (368, 347), (356, 341), (343, 334), (349, 335), (370, 335), (379, 338), (387, 339), (406, 348)]}]\n"
     ]
    }
   ],
   "source": [
    "print(face_recognition.face_locations(imgS))\n",
    "print(face_recognition.face_landmarks(imgS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee004b54-449e-47e1-96c4-d5e51c8b5c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chimya', 'png']\n",
      "['girish', 'jpg']\n",
      "['momo', 'jpeg']\n"
     ]
    }
   ],
   "source": [
    "train_path = 'train_img_path'\n",
    "try_path = os.listdir(train_path)\n",
    "member_Names = []\n",
    "\n",
    "for file in try_path:\n",
    "  if(file.endswith(\".jpeg\") or file.endswith(\".jpg\") or file.endswith(\".png\")):\n",
    "    print(file.split('.'))\n",
    "    name_with_ext = file.split('.')\n",
    "    member_Names.append(name_with_ext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a31ef931-e23e-4735-9fe6-bbc3bc8c2c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chimya', 'girish', 'momo']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "member_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17c6bdc5-93da-4ba8-abf2-bec33879cae7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'Attendance.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(img, (x1,y2\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m35\u001b[39m),(x2,y2),(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m0\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFILLED)\n\u001b[0;32m     21\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(img, name, (x1\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m6\u001b[39m, y2\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_COMPLEX, \u001b[38;5;241m1\u001b[39m,(\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m),\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mmarkAttendance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebcam\u001b[39m\u001b[38;5;124m'\u001b[39m,img)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mmarkAttendance\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmarkAttendance\u001b[39m(name):\n\u001b[1;32m----> 2\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAttendance.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     dataList \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      4\u001b[0m     nameList \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'Attendance.csv'"
     ]
    }
   ],
   "source": [
    "# Should work on local system\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "  success, img = capture.read()\n",
    "  # imgS = cv2.resize(img, (0,0), None, 0.25,0.25)\n",
    "  faces_frame = face_recognition.face_locations(img)\n",
    "  encoded_faces = face_recognition.face_encodings(img, faces_frame)\n",
    "  for encode_face, faceloc in zip(encoded_faces, faces_frame):\n",
    "    matches = face_recognition.compare_faces(encode_face_List, encode_face)\n",
    "    faceDist = face_recognition.face_distance(encode_face_List, encode_face)\n",
    "    matchIndex = np.argmin(faceDist)\n",
    "    print(matchIndex)\n",
    "    if(matches[matchIndex]):\n",
    "      name = member_Names[matchIndex].upper().lower()\n",
    "      y1,x2,y2,x1 = faceloc\n",
    "\n",
    "      # y1,x2,y2,x1 = y1*4, x2*4, y2*4, x1*4\n",
    "      cv2.rectangle(img, (x1,y1),(x2,y2),(0,255,0),2)\n",
    "      cv2.rectangle(img, (x1,y2-35),(x2,y2),(0,255,0), cv2.FILLED)\n",
    "      cv2.putText(img, name, (x1+6, y2-5), cv2.FONT_HERSHEY_COMPLEX, 1,(255,255,255),2)\n",
    "      markAttendance(name)\n",
    "  cv2.imshow('webcam',img)\n",
    "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
